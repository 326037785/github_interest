quantize
https://github.com/eladhoffer/quantized.pytorch
使用pytorch

我们真的需要模型压缩吗？
http://mitchgordon.me/machine/learning/2020/01/13/do-we-really-need-model-compression.html

### [ECCV 2018] PyTorch implementation for AMC: AutoML for Model Compression and Acceleration on Mobile Devices.
https://github.com/mit-han-lab/amc

神经网络量化相关文献集
https://github.com/xu3kev/neural-networks-quantization-notes

Pytorch Implementation of Neural Architecture Optimization
https://github.com/renqianluo/NAO_pytorch

XNNPACK：面向手机和浏览器的高效浮点神经网络推理运算器
https://github.com/google/XNNPACK

TensorRT 深度学习优化
https://github.com/ardianumam/Tensorflow-TensorRT

prune
https://github.com/jacobgil/pytorch-pruning
https://jacobgil.github.io/deeplearning/pruning-deep-learning
2016年的算法

https://github.com/Eric-mingjie/rethinking-network-pruning
提供了6种网络剪枝方法，使用pytorch
https://github.com/alexfjw/prunnable-layers-pytorch

Pruning AI networks without impacting performance
https://github.com/DNNToolBox/Net-Trim-v1

基于Keras的AutoML机器学习自动化库
https://github.com/jhfjhfj1/autokeras

prune & quantize
https://github.com/NervanaSystems/distiller
intel开源的网络蒸馏工具，使用pytorch框架，不断更新中。。。
教程：https://github.com/NervanaSystems/distiller/wiki/Tutorial:-Using-Distiller-to-prune-a-PyTorch-language-model

tencent
https://github.com/Tencent/PocketFlow

自动化超参数搜索，后面可接很多不同框架
https://github.com/tobegit3hub/advisor

网络结构搜索，谷歌出品
https://github.com/tensorflow/adanet

Slimmable Neural Networks
https://github.com/JiahuiYu/slimmable_networks
https://github.com/JiahuiYu/slimmable_networks/tree/detection

Trained Rank Pruning for Efficient Deep Neural Networks
https://github.com/yuhuixu1993/Trained-Rank-Pruning

AutoML相关资源列表
https://github.com/dragen1860/awesome-AutoML

深度网络模型压缩与加速相关文献大列表
https://github.com/sun254/awesome-model-compression-and-acceleration

超参优化框架Optuna
https://github.com/pfnet/optuna

微软发布的AutoML工具包(自动网络结构搜索/超参优化)
https://github.com/Microsoft/nni
https://github.com/Microsoft/nni/blob/master/docs/GetStarted.md

Keras模型超参调优工具
https://github.com/autonomio/talos

神经网络机器学习自动化框架
https://github.com/CiscoAI/amla

PyTorch 实现的NEAT (NeuroEvolution of Augmenting Topologies)神经进化算法
https://github.com/uber-research/PyTorch-NEAT/

模型超参自动搜索工具
https://github.com/NVIDIA/Milano

神经网络模型量化方法简介
http://chenrudan.github.io/blog/2018/10/02/networkquantization.html

Rethinking the Value of Network Pruning
https://github.com/Eric-mingjie/rethinking-network-pruning

在线超参数优化平台Bender
https://github.com/Dreem-Organization/benderopt


Tensorflow Implementation of ChannelNets (NIPS18) https://arxiv.org/abs/1809.01330
https://github.com/HongyangGao/ChannelNets

Neural Architecture Optimization
https://github.com/renqianluo/NAO

AdaNet 简介：快速灵活的 AutoML，提供学习保证
https://github.com/tensorflow/adanet
https://mp.weixin.qq.com/s?__biz=MzU1OTMyNDcxMQ==&mid=2247485095&idx=1&sn=990ca481921261e9ff5d850bd3753d6e&chksm=fc184defcb6fc4f986862449c630c80e9f285647a6bcac4a9e03c1108d11554a427d62da148e&scene=0&xtrack=1#rd

Sonnet是个基于TensorFlow的库，可以帮助你建立复杂的神经网络。该项目由Deepmind的Malcolm Reynolds创建。
https://github.com/deepmind/sonnet

神经网络简化应用框架
https://github.com/mindsdb/mindsdb

腾讯发布的模型压缩自动化(AutoMC)框架
https://github.com/Tencent/PocketFlow

面向手机优化的量化神经网络算子库
https://github.com/pytorch/QNNPACK

NVIDIA TensorRT：NVIDIA GPU和深度学习加速器的C++高性能推理库
https://github.com/NVIDIA/TensorRT

PeleeNet: An efficient DenseNet architecture for mobile devices
https://github.com/Robert-JunWang/PeleeNet

深度网络压缩/加速最新进展列表
https://github.com/MingSun-Tse/EfficientDNNs

用于寻找和分析神经网络重要神经元的工具包
https://github.com/fdalvi/NeuroX

开源:Keras + Hyperopt方便超参优化的简单封装Hyperas
http://maxpumperla.com/hyperas/

PyTorch机器学习自动化：自动框架搜索、超参优化
https://github.com/automl/Auto-PyTorch

Tensorized Embedding Layers for Efficient Model Compression
https://github.com/KhrulkovV/tt-pytorch

### 基于pytorch实现模型压缩
https://github.com/666DZY666/model-compression

DeepSpeed：微软的深度学习优化库，让分布式训练更简单、更高效、更有效
https://github.com/microsoft/DeepSpeed
https://github.com/microsoft/DeepSpeedExamples

Adlik：深度学习模型端到端优化框架，其模型编译器支持剪枝、量化和结构压缩等多种优化技术，可以方便地用于使用 TensorFlow、 Keras、 PyTorch 等开发的模型，服务平台提供基于部署环境的具有优化运行时的深度学习模型
https://github.com/Adlik/Adlik

Code for paper "Learning to Reweight Examples for Robust Deep Learning"
https://github.com/uber-research/learning-to-reweight-examples

Code for “Discrimination-aware-Channel-Pruning-for-Deep-Neural-Networks”
https://github.com/SCUT-AILab/DCP

ZeroQ: A Novel Zero Shot Quantization Framework
https://github.com/amirgholami/ZeroQ

Graph Transforms to Quantize and Retrain Deep Neural Nets in TensorFlow. https://arxiv.org/abs/1903.08066
https://github.com/Xilinx/graffitist

A PyTorch implementation of "Incremental Network Quantization: Towards Lossless CNNs with Low-Precision Weights"
https://github.com/Mxbonn/INQ-pytorch

This repository contains the training code of Quantization Networks introduced in our CVPR 2019 paper: Quantization Networks.
https://github.com/aliyun/alibabacloud-quantization-networks

Code released for "FNNP: Fast Neural Network Pruning Using Adaptive Batch Normalization"
https://github.com/anonymous47823493/FNNP

Code for the NuerIPS'19 paper "Gate Decorator: Global Filter Pruning Method for Accelerating Deep Convolutional Neural Networks"
https://github.com/youzhonghui/gate-decorator-pruning

【PyTorch实现的深度模型压缩】
https://github.com/666DZY666/model-compression

【深度网络压缩文献/代码列表】
https://github.com/csyhhu/Awesome-Deep-Neural-Network-Compression

【模型压缩相关文献资源大列表】
https://github.com/ChanChiChoi/awesome-model-compression

【神经网络压缩与加速资源集锦】
https://github.com/mrgloom/Network-Speed-and-Compression

Neural Rejuvenation: Improving Deep Network Training by Enhancing Computational Resource Utilization at CVPR'19
https://github.com/joe-siyuan-qiao/NeuralRejuvenation-CVPR19

### 面向目标检测/语义分割的机器学习自动化(AutoML)
https://github.com/NoamRosenberg/AutoML

【神经网络修剪技术研究指南】
https://pan.baidu.com/s/1onGzAUw4pKrySM1uS1HTeg

机器学习模型压缩相关文献、工具、学习资料列表
https://github.com/cedrickchee/awesome-ml-model-compression

深度神经网络修剪
https://towardsdatascience.com/pruning-deep-neural-network-56cae1ec5505

面向图像分类和检测的神经网络压缩
https://arxiv.org/abs/1907.05686 https://ai.facebook.com/blog/compressing-neural-networks-for-image-classification-and-detection/

Partial Channel Connections for Memory-Efficient Differentiable Architecture Search
https://github.com/yuhuixu1993/PC-DARTS

Code for: "And the bit goes down: Revisiting the quantization of neural networks"
https://github.com/facebookresearch/kill-the-bits

Implementation with latest PyTorch for multi-gpu DARTS https://arxiv.org/abs/1806.09055
https://github.com/alphadl/darts.pytorch1.1 https://github.com/quark0/darts

神经网络架构搜索相关资源大列表
https://github.com/D-X-Y/awesome-NAS