著名风投Andreessen-Horowitz对“人工智能”初创企业的高度点评：云基础设施是AI公司的重要隐性成本，要获得边际收益，需要付出很高的深度学习计算成本；许多AI应用对人工依赖程度很高；初创公司通常没什么护城河或秘密武器；创业公司主要提供服务，而不是软件业务；机器学习在数据和流程效率低下的大型组织中最有生产力
https://scottlocklin.wordpress.com/2020/02/21/andreessen-horowitz-craps-on-ai-startups-from-a-great-height/

不要把人类看作是万物的王冠。 相反，把人类文明看作是一个更宏大计划的一部分，是宇宙走向更高复杂性道路上的重要一步。

Nikhil Thorat：软件工程师和研究人员喜欢算法问题(包括深度学习库) ，因为这些问题干净又纯粹ーー但现实世界的数据并非如此。这为人们构建工具来理解和处理脏数据留出了巨大的机会。 ​​​​

François Chollet：一种描述智能的方法，是“从片面描述中理解事物的能力”。这种简单的描述，可以作为智能的一个评价标准。如果你的AI，需要极其详尽的任务描述(用明确的手工编写的程序，或用静态数据集密集采样) ，它实际上并是不智能的。当你操作它的时候，你只是在查询你输入过的信息，而非它的自主反应。智能是AI将任务信息转化为任务技能的比率。重要的是，任务通常不是静态的，所以这里的“技能”针对的是未来的情况——可能和过去的情况截然不同。转化，是从过去的操作空间到未来的操作空间。

“古今之成大事业、大学问者，必经过三种之境界：‘昨夜西风凋碧树，独上高楼，望尽天涯路’。此第一境也。‘衣带渐宽终不悔，为伊消得人憔悴。’此第二境也。‘众里寻他千百度，蓦然回首，那人却在灯火阑珊处’。此第三境也。” - 王国维《人间词话》 ​​​​
想做大事，首先要跳出纷繁的世界，经历孤独的思考，看到真正想要的东西——跳出纷扰，找到目标；苦苦求索，反复摹想，付出一切也绝不后悔，不会为短期得失斤斤计较，也不会为最终无果而悔恨——孜孜不倦，无悔追寻；最好的结果，是在历经寻找、历经思考以后的顿悟。

【数据之惑：One-Shot & Zero-Shot Learning】《The Data Problem II: One-Shot and Zero-Shot Learning | Synthesis AI》
https://synthesis.ai/2020/03/30/the-data-problem-ii-one-shot-and-zero-shot-learning/

【数据之惑：未标注数据有用吗？】《The Data Problem IV: Can Unlabeled Data Help? | Synthesis AI》
https://synthesis.ai/2020/04/14/the-data-problem-iv-can-unlabeled-data-help/


François Chollet：哲学经常受到科技界人士的嘲笑。但我很喜欢。我所做过最有价值的事，都和“哲学”大有渊源。 哲学强大、可行、实用，又与众不同。哲学的真正目的，不是得出具体的理论(理论反映的，往往时所处时代的局限) ，或教给你有用的“生活技巧”。 哲学不是科学，也不是工程。 哲学的目的，是为你提供思考的基石。读哲学的价值，不在于你能记住什么概念或理论，而在于它们对你未来思维方式的拓展。就像学编程的价值，不在于你能写出什么样的小程序，而在于对你思维方式的刷新。

François Chollet：机器学习最有趣的应用，往往来自于没多少技术背景的人。 科技与艺术的交叉，是人性最能闪光的地方。 ​​​​

爱因斯坦：美在本质上终究是简单性。

Jacob Menick：一旦你意识到，机器学习模型只是数据的一个视图，你就会真正开始对数据集感兴趣。 

如何做科研？
• 沉迷于发现的快乐
• 拥抱不确定性，勇于质疑
• 把每一个问题，当作一次机会
• 对所有事物保持好奇
• 热爱细节
• 不断思考
• 深度专注
• 拒绝“天才”说

多读论文多思考。感觉迷茫、没想法，要么是文献读的不够多，要么是没有真正去思考，或角度不对。创新，一定是在充分了解的基础上——不只看别人做了什么，要看别人是怎么找问题、找角度的，认真做一次文献综述，一定会有所收获。问题对了，就已经成功了一半。

François Chollet：模型擅长预测过去，而未来才是他们要去面对的。

以前与Yann LeCun聊天，他谈到神经网络为什么现今如此流行的原因，是因为所有重要的ideas都是三十年前的，已经没有任何新的ideas可以被patent，大家都可以用。而这一轮深度学习的主要贡献是对这些ideas的集成和实现。当然，留给我们做理论的就是解释、简化和统一！

【Marcus访谈：AI的希望、隐患与未来】
https://jamanetwork.com/journals/jama/article-abstract/2766942

Sam Altman：AI工具会干掉很多工作(设计、数据科学、编程、业务邮件处理等)最枯燥乏味的部分。可以预见：当我们再也不用破坏节奏、慢下来干那些重复、乏味的任务，工作效率将会大大提高，这种变化将是非常惊人的。 ​​​​

@毅马当闲 最近越来越多的证据表明，通过拟合数据得到的深度神经网络模型（在classification，detection，segmentation等）对输入很小的数值扰动和很小的变换deformation（甚至平移）都是不稳定的（unstable)，更谈不上鲁棒。不要再相信别人show的成功例子 -- 我过去就是被别人show的一些例子迷糊，有些相信这样的模型（通过在augmented数据上训练）会是稳定的，而自己没有去做严格的验证 -- 肠子都有些悔青了。但应该不会再被忽悠了。所以目前基于深度学习的“人工智能”，用在不痛不痒的应用上，也就罢了。把这样的模型用在严肃的问题上（例如需要有安全、隐私、可靠性保障的），应该是十分危险的。虽然这并不是说，通过系统严格的改进，深度模型和算法就不能没有性能上保障。但那需要建立一套完整的理论体系，正确的模型需要推导出来（而不是试错出来），而其性能保证也必须要有严格的证明。其实不少顶尖的研究人员都已经意识到这一点，今后几年，大家应该会看到系统的理论研究的强势回归。不会让深度学习把传统工程理论已经得到的常识和教训再从新发明一遍。

@毅马当闲 有趣的是我们最近的工作发现机器学习(深度学习)与sphere-packing问题本质上是一致的。目前的机器学习就是指导机器如何把数据紧致有效地pack到高维空间中。不像数学家求closed-form解，但求有效的数值逼近最优解。其实信息理论coding theory的基础，也是sphere packing。任何学问，简单到了极致，才是本质。

François Chollet：AI的发展方向，应该是理解，而非模仿。要理解生命的本质，就该看看自创生(autopoiesis)、分子生物学、DNA；要模仿生命，可以画卡通画。两条路截然不同。大量的“人工智能”，其实就是神经卡通片。能*理解*的人工智能，不会从一开始就看起来很聪明的样子，但它会逐渐扩展到那个层次——直到和我们一样。卡通片看起来像真的，但实际上没啥真东西。卡通片只能放你画的场景，但生命体会以开放的方式自适应，生命体是完全自主的。

【最重要的深度学习思想——简要历史回顾(深度学习推荐论文列表)】
https://dennybritz.com/blog/deep-learning-most-important-ideas/

François Chollet：理想：如果我们发明出强人工智能，就可以用它来解决其他所有问题。现实：如果我们不能找到解决病毒式假消息的方法，就根本解决不了任何问题。 ​​​​

最好的学习者，是那些能克服客观面对不擅长某事所产生的不适感的人。 - Tommy Collison ​​​​

François Chollet：关于机器学习，首先要记住，性能是用数据集里的样本评价的，模型在生产环境面对的样本，很可能大相径庭……对此，金融业有种说法: “过去的表现并不能保证未来的结果”。模型在测试数据集上得到X分，并不代表放到现实世界，在接下来的N种情况下也能达到X。未来与过去，可能有很大差异。因此，当问到“你更愿意用90%准确率的模型，还是80%准确率的人”时，答案取决于用来评价的数据是否够典型。人有强大的适应能力，而模型没有。如果存在重大的不确定性，就选择人类，他们模式识别能力可能不那么强大(相对于用大量数据训练出来的模型)，但他们知道自己在做什么，可以自行推理，面对新奇事物时，还能即兴发挥。如果每种可能的情况都已经掌握，并且希望优先考虑可扩展性、降低成本，就选择模型。 模型的存在，就是为了在理解透彻的情况下，对人类认知进行编码和操作化。(“理解透彻”的意思是，程序员可以确切地描述出来，或者可以积累一个数据集，通过密集采样捕获所有可能情况的真实分布——而且只能是静态的)


![](https://arloseimg.oss-cn-hangzhou.aliyuncs.com/20200508115035.png)

![](https://arloseimg.oss-cn-hangzhou.aliyuncs.com/20200508172532.png)

学生提了2000个问题后，刘云浩教授又写万字回复
https://weibo.com/ttarticle/p/show?id=2309404537496498471101

深度学习未来发展的三种学习范式：混合学习、成分学习和简化学习
https://towardsdatascience.com/the-future-of-deep-learning-can-be-broken-down-into-these-3-learning-paradigms-e7970dec5502

【无监督表示学习】Exploring Simple Siamese Representation Learning
本文是何恺明关于无监督表示学习的一篇新工作，非常值得一读。本文主要针对应用非常普遍的孪生网络（Siamese Network）进行分析，以目前非常火的对比学习为例，孪生网络使用一个相同的网络处理同一个输入的两个不同表示，通过拉近两个positive pair的表示，拉远两个negative pair之间的表示，从而学习到输入中的不变性，从而更好的学习到输入的表示。而本文通过实验分析得出在孪生网络中发挥最重要作用的就是孪生网络的结构，其他一些方法的作用并没有那么大。除此之外，作者还提出了一种“stop-gradient”的算法，该算法主要对模型的loss反馈时，通过梯度终止的机制，使得只更新其中一个encoder，实现了对孪生网络中的崩溃解（collapsing）很好的避免。而且这种简单的结构能够在ImageNet和下游任务取得非常好的效果。为了证明这种算法的有效性，作者进行了大量的实验，充分证明该算法的优越性。而且作者还深入讨论了文中提出的算法到底在优化模型的哪些地方。方法简单，效果有效，值得认真读一下的大作
http://www.paperweekly.site/papers/4652

乔姆斯基：深度学习的未来
https://towardsdatascience.com/noam-chomsky-on-the-future-of-deep-learning-2beb37815a3e

陶大程院士等最新《深度学习理论进展》综述论文，41页pdf255篇文献阐述六大方面进展
https://mp.weixin.qq.com/s/_Rw_iv6UuXyrsBvTYkIwbA?v_p=86&WBAPIAnalysisOriUICodes=10000001&launchid=10000365--x&wm=3333_2001&aid=01AqqH81ztmhYksgMY-LlLzIKVJrvyH6eOULNkkN2WGVj-VUM.&from=10AC393010

在失败中学习，MIT新研究显示，机器可以像婴儿一样学会理解人类目标 
人的认知中有一个叫做错误探测。不停探测所有可能，然后抛弃错误的。，留下有用的。。
https://www.aminer.cn/research_report/5fe8067ce8a87f775ad223bf?download=false

深度学习的三个秘密：集成、知识蒸馏和自蒸馏
https://www.microsoft.com/en-us/research/blog/three-mysteries-in-deep-learning-ensemble-knowledge-distillation-and-self-distillation/

讨论：大多机器学习研究只是现有模型和数据集的排列组合？
https://www.reddit.com/r/MachineLearning/comments/l2q3hh/research_most_ml_research_is_just_permutations/

Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks
https://arxiv.org/abs/2102.00554

This repo provides the scripts to test a learned AlexNet's feature representation performance at the five different convolutional levels -- in parallel.
https://github.com/yukimasano/linear-probes

Learning error bars for neural network predictions
https://github.com/facebookresearch/SingleModelUncertainty

Gradient Estimation with Stochastic Softmax Tricks
https://github.com/choidami/sst

An essential implementation of BYOL in PyTorch + PyTorch Lightning
https://github.com/DonkeyShot21/essential-BYOL

Rethinking soft labels for knowledge distillation: a bias-variance tradeoff perspective
https://github.com/bellymonster/Weighted-Soft-Label-Distillation

Learning to Initialize Neural Networks for Stable and Efficient Training
https://github.com/zhuchen03/gradinit

Code based on the ICLR 2021 paper Can a Fruit Fly Learn Word Embeddings?.
https://github.com/bhoov/flyvec

揭示机器学习中未知的未知
https://ai.googleblog.com/2021/02/uncovering-unknown-unknowns-in-machine.html

AI的需求层次
https://medium.com/hackernoon/the-ai-hierarchy-of-needs-18f111fcc007

HYDRA: Hypergradient Data Relevance Analysis for Interpreting Deep Neural Networks
https://github.com/cyyever/aaai_2021_hydra

Perceiver: General Perception with Iterative Attention
https://www.arxiv-vanity.com/papers/2103.03206

### 《Involution: Inverting the Inherence of Convolution for Visual Recognition》(CVPR 2021) 
https://github.com/d-li14/involution 

理解深度学习泛化的新视角
https://ai.googleblog.com/2021/03/a-new-lens-on-understanding.html

OpenAI最先进的机器视觉AI被手写纸条愚弄了——机器与人类智能的案例研究
https://www.theverge.com/2021/3/8/22319173/openai-machine-vision-adversarial-typographic-attacka-clip-multimodal-neuron

Involution: Inverting the Inherence of Convolution for Visual Recognition
github.com/d-li14/involution

Is it Enough to Optimize CNN Architectures on ImageNet?
https://www.arxiv-vanity.com/papers/2103.09108

《Understanding Robustness of Transformers for Image Classification》
https://www.arxiv-vanity.com/papers/2103.14586

《Explainable Deep One-Class Classification》(ICLR 2021) 
github.com/liznerski/fcdd

《A Fourier Perspective on Model Robustness in Computer Vision》(NeurIPS 2019) 
github.com/gatheluck/FourierHeatmap

Michael I. Jordan：现在的“AI技术”还未触及高层次的推理和思考，距离“智能”还很远，“尽管科学为人类做了许多美妙的事，但真正能最直接、最深刻地增加人类幸福感的，是工程学”
https://spectrum.ieee.org/the-institute/ieee-member-news/stop-calling-everything-ai-machinelearning-pioneer-says

Lighting the Darkness in the Deep Learning Era》(2021)
github.com/Li-Chongyi/Lighting-the-Darkness-in-the-Deep-Learning-Era-Open

关于机器学习可解释性的几个报告：
“Explaining Machine Learning Predictions: State-of-the-art, Challenges, and Opportunities(NeurIPS 2020 Tutorial)” https:// explainml-tutorial.github.io/neurips20
“Explaining Machine Learning Predictions: State-of-the-art, Challenges, and Opportunities(AAAI 2021 Tutorial)” https:// explainml-tutorial.github.io/aaai21
“When Not to Trust Your Explanations” https:// docs.google.com/presentation/d/10a0PNKwoV3a1XChzvY-T1mWudtzUIZi3sCMzVwGSYfM/edit#slide=id.p
“Explainable Machine Learning:
Understanding the Limits & Pushing the Boundaries” https:// drive.google.com/file/d/1xn2dCDAeEEhB_rex202KxMPqIPj31fZ4/view

What Kinds of Functions do Deep Neural Networks Learn? Insights from Variational Spline Theory
https://www.arxiv-vanity.com/papers/2105.03361

On Robustness and Transferability of Convolutional Neural Networks
https://arxiv.org/abs/2007.08558

讨论：representation vs. latent vs. embedding
www.reddit.com/r/MachineLearning/comments/ofivs2/d_difference_between_representation_vs_latent_vs/h4d0fcj/

从视觉到语言：(Google搜索引擎)半监督学习(蒸馏)的大规模实用
https://ai.googleblog.com/2021/07/from-vision-to-language-semi-supervised.html

AI产业“入行须知”四则
towardsdatascience.com/4-things-i-wish-i-knew-before-starting-in-the-ai-industry-5458c6bf48b9
“并非所有带有"AI"一词的东西都是真正的AI，一些公司利用它背后的营销力量来吸引投资者、客户或员工，一定要当心……AI不只是深度学习……媒体总是过度夸大，误导读者……试着去看第一手材料，自己做出判断……人工智能既是一项工程技术，同时也是一门科学学科”

Jeremy Howard访谈：Kaggle, Enlitic, 与 fast.ai 
https://thegradientpub.substack.com/p/jeremy-howard-on-kaggle-enlitic-and

Stephen Hanson vs. Michael Jordan：AI革命是否正在到来？
https://www.bilibili.com/video/BV1R64y1b77V/

斯坦福AI状况报告
https://ai100.stanford.edu/2021-report/gathering-strength-gathering-storms-one-hundred-year-study-artificial-intelligence?sf151132193=1

François Chollet：关于机器学习的一个普遍误解是，模型是中立/客观的，反映的只是用来训练的数据。建模方式的选择是在对数据进行了强假设之后，在应用中使用模型实际上反映了人对问题的理解。 ​​​ ​​​​

人工智能软件创业公司调研
github.com/WarrenWen666/AI-Software-Startups

要：用两年前的算法、最可靠的实现，标注更多的数据。
不要: 拿最新的NeurIPS论文，声称超出SOTA 1%，从头实现，可能永远没法复现。 ​​​
via:Andriy Burkov ​​​​

2021机器学习、人工智能和数据(MAD)全景图
https://mattturck.com/data2021/

关于AI难以忽视的事实
https://spectrum.ieee.org/rodney-brooks-ai
现在成功的AI 项目：1）都有人来兜底； 2）即使失败也无伤大雅

Do Self-Supervised and Supervised Methods Learn Similar Visual Representations?
https://arxiv.org/abs/2110.00528

IEEE Spectrum杂志特刊：AI大变局——总结与反思
https://spectrum.ieee.org/special-reports/the-great-ai-reckoning/

Patches Are All You Need? 
github.com/tmp-iclr/convmixer

Can Vision Transformers Perform Convolution?》
https://arxiv.org/abs/2111.01353

2021 AI大事记：AI“突破性”成果相关文章、讲解视频及代码列表
github.com/louisfb01/best_AI_papers_2021

《Are Transformers More Robust Than CNNs?》
github.com/ytongbai/ViTs-vs-CNNs

Tabular Data: Deep Learning is Not All You Need
https://arxiv.org/abs/2106.03253

像开发开源软件一样构建开放可重用增量更新的机器学习模块
https://colinraffel.com/blog/a-call-to-build-models-like-we-build-open-source-software.html

机器学习方法：可用性 vs. 可理解性
https://www.aidancooper.co.uk/utility-vs-understanding/

您真正需要关注的AI 技术风向
https://mp.weixin.qq.com/s/ZQVdQPd9St8CgiJJEx3mLg

imodels：可解释机器学习包，用于简洁、透明和准确的预测建模
github.com/csinva/imodels 

Sam Gershman：所有为可解释AI绞尽脑汁的人，都该看看关于人们如何解释自己行为的认知科学文献。有很多非常详细的解释。可唯一的问题是，这些解释可能非常不靠谱！我们要求从AI获得一种可解释性，如果我们连解释自己的行为都做不到，又怎么去要求别人。我觉得这些错觉是模块化系统(例如大脑)通信瓶颈的必然结果。不绕开这个模块化架构，大脑的一部分无法解释大脑另一部分所做的一切。这些瓶颈是可取的，因为有助于实现分工。出于同样的原因，我们希望人机交互中也存在瓶颈。这就需要对可解释性做出限制。
爱可可-爱生活：很多时候，我们希望的可解释不是为了*说得通*，而是为了*搞得定*——可以不必知道为什么这样做决策，可一旦出了问题，需要有明确可理解的线索，去追查问题出在哪、在哪打补丁做优化能搞定，比如人脸识别，把某些肤色识别成猩猩的“突发事故”要应急处理，做不到可解释，只能全面下线先。

Visual Attention Network
https://arxiv.org/abs/2202.09741

人工智能可解释性研究的前沿问题
edium.com/@ alonjacovi/frontier-questions-in-ai-explainability-research-2363991b7116

深度学习正面临“瓶颈”
https://nautil.us/deep-learning-is-hitting-a-wall-14467/

Transformer能否一统AI江湖？
www.quantamagazine.org/will-transformers-take-over-artificial-intelligence-20220310

Deep Learning 2.0: Extending the Power of Deep Learning to the Meta-Level
https://www.automl.org/deep-learning-2-0-extending-the-power-of-deep-learning-to-the-meta-level/

从第一性原理看深度学习成功的关键
https://horace.io/brrr_intro.html

机器学习的潜力与局限
https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained

Alex Dimakis：相对于经典机器学习模型，深度学习有个不常被提到的巨大优势，就是*模块化*：可以下载各种预训练模型，像拼乐高一样把它们拼在一起，因为梯度的连续流动，让端到端的微调得以实现。

Visual Prompting: Modifying Pixel Space to Adapt Pre-trained Models
https://arxiv.org/abs/2203.17274
![](https://wx3.sinaimg.cn/mw690/5396ee05ly1h0w2k6czzrj20zt0z6tj4.jpg)

![](https://arloseimg.oss-cn-hangzhou.aliyuncs.com/20220502110853.png)

进化能估计梯度吗？
joramkeijser.github.io/2022/05/01/mutations.html 