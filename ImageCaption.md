Recognition to Cognition Networks https://visualcommonsense.com
https://github.com/rowanz/r2c

CVPR 2018 - Regularizing RNNs for Caption Generation by Reconstructing The Past with The Present
https://github.com/chenxinpeng/ARNet

Beyond Narrative Description: Generating Poetry from Images by Multi-Adversarial Training
https://github.com/bei21/img2poem

Dataset and starting code for visual entailment dataset https://arxiv.org/abs/1811.10582
https://github.com/necla-ml/SNLI-VE

Mapping Images to Scene Graphs with Permutation-Invariant Structured Prediction
https://github.com/shikorab/SceneGraph

### 【视觉/语言预训练模型最新进展】'Recent Advances in Vision and Language PreTrained Models (VL-PTMs)'
https://github.com/yuewang-cuhk/awesome-vision-language-pretraining-papers

Aligning Visual Regions and Textual Concepts for Semantic-Grounded Image Representations
https://github.com/fenglinliu98/MIA

Tensorflow implementation of "A Structured Self-Attentive Sentence Embedding"
https://github.com/flrngel/Self-Attentive-tensorflow

This repository contains the reference code for the paper Show, Control and Tell: A Framework for Generating Controllable and Grounded Captions (CVPR 2019).
https://github.com/aimagelab/show-control-and-tell 

The Code for ICME2019 Grand Challenge: Short Video Understanding (Single Model Ranks 6th)
https://github.com/guoday/ICME2019-CTR

【基于Transformer的图像自动描述PyTorch/Fairseq扩展】
https://github.com/krasserm/fairseq-image-captioning

Code for Neural Inverse Knitting: From Images to Manufacturing Instructions
https://github.com/xionluhnis/neural_inverse_knitting

Code for paper "Attention on Attention for Image Captioning". ICCV 2019 https://arxiv.org/abs/1908.06954
https://github.com/husthuaan/AoANet

Learning to Evaluate Image Captioning. CVPR 2018
https://github.com/richardaecn/cvpr18-caption-eval

Vision-Language Pre-training for Image Captioning and Question Answering
https://github.com/LuoweiZhou/VLP

Official Tensorflow Implementation of the paper "Bidirectional Attentive Fusion with Context Gating for Dense Video Captioning" in CVPR 2018, with code, model and prediction results.
https://github.com/JaywongWang/DenseVideoCaptioning

A PyTorch implementation of Transformer in "Attention is All You Need" https://arxiv.org/abs/1706.03762
https://github.com/dreamgonfly/Transformer-pytorch

《ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data》
https://www.arxiv-vanity.com/papers/2001.07966/

【结合BERT的图片描述生成】’Image Captioning System - BERT + Image Captioning'
https://github.com/ajamjoom/Image-Captions

M^2: Meshed-Memory Transformer for Image Captioning
https://github.com/aimagelab/meshed-memory-transformer

### Video Grounding and Captioning
https://github.com/facebookresearch/grounded-video-description

Reformer：高效的Transformer
https://github.com/google/trax/tree/master/trax/models/reformer

ICCV研讨会的中英文视频描述大赛
http://vatex.org/main/index.html

Cooperative Vision-and-Dialog Navigation
https://github.com/mmurray/cvdn

Auto-Encoding Scene Graphs for Image Captioning, CVPR 2019
https://github.com/yangxuntu/SGAE

A PyTorch implementation of the Transformer model from "Attention Is All You Need".
https://github.com/phohenecker/pytorch-transformer

【MMF：基于PyTorch的视觉/语言研究模块化框架，可方便进行VQA、图像描述、视觉对话、仇恨检测和其他视觉/语言任务的研究】
https://github.com/facebookresearch/mmf

基于transformers的图像Instagram标题生成
https://github.com/antoninodimaggio/Hugging-Captions

Implementation of 'X-Linear Attention Networks for Image Captioning' [CVPR 2020]
https://github.com/JDAI-CV/image-captioning

Train Scene Graph Generation for Visual Genome and GQA in PyTorch >= 1.2 with improved zero and few-shot generalization. Paper: "Graph Density-Aware Losses for Novel Compositions in Scene Graph Generation"
https://github.com/bknyaz/sgg

Code and Resources for the Transformer Encoder Reasoning Network (TERN) - https://arxiv.org/abs/2004.09144
https://github.com/mesnico/TERN

Code for ACL 2020 paper "Dense-Caption Matching and Frame-Selection Gating for Temporal Localization in VideoQA."
https://github.com/hyounghk/VideoQADenseCapFrameGate-ACL2020

[ACL 2020] PyTorch code for MART: Memory-Augmented Recurrent Transformer for Coherent Video Paragraph Captioning
https://github.com/jayleicn/recurrent-transformer

PyTorch code for: Learning to Generate Grounded Visual Captions without Localization Supervision
https://github.com/chihyaoma/cyclical-visual-captioning

Say As You Wish: Fine-grained Control of Image Caption Generation with Abstract Scene Graphs
https://github.com/cshizhe/asg2cap

Fine-grained Video-Text Retrieval with Hierarchical Graph Reasoning
https://github.com/cshizhe/hgr_v2t

The repository of ECCV 2020 paper Active Visual Information Gathering for Vision-Language Navigation
https://github.com/HanqingWangAI/Active_VLN

Code for the CVPR 2020 oral paper: Weakly Supervised Visual Semantic Parsing
https://github.com/alirezazareian/vspnet https://arxiv.org/abs/2001.02359

Learning Visual Representations with Caption Annotations
https://arxiv.org/abs/2008.01392


Towards Unique and Informative Captioning of Images
https://github.com/princetonvisualai/SPICE-U

计算机如何做到“看图说话”？在没有对应的训练数据的情况下，模型能否准确描述测试图像中新出现的各种类别的物体？微软 Azure 认知服务团队和微软研究院的研究员提出了全新解决方案视觉词表预训练 (Visual Vocabulary Pre-training)。该方法在 nocaps 挑战中取得了新的 SOTA，并首次超越人类表现
https://weibo.com/ttarticle/p/show?id=2309404559989682602061

一个视频的文本摘要生成项目，输入一段视频，通过深度学习网络和人工智能程序识别视频主要表达的意思
https://github.com/CaptainEven/VideoCaption

An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale
https://github.com/google-research/vision_transformer

### CapWAP: Captioning with a Purpose
https://arxiv.org/abs/2011.04264
https://github.com/google-research/language/tree/master/language/capwap

Transformer视觉表示学习文献资源列表
https://github.com/alohays/awesome-visual-representation-learning-with-transformers

Online Bag-of-Visual-Words Generation for Unsupervised Representation Learning
https://github.com/valeoai/obow

VinVL: Making Visual Representations Matter in Vision-Language Models
https://arxiv.org/abs/2101.00529

Cross-Document Language Modeling
https://github.com/aviclu/CD-LM https://arxiv.org/abs/2101.00406

Transformers in Vision: A Survey
https://arxiv.org/abs/2101.01169

视觉Transformer相关工作列表
https://github.com/dk-liang/Awesome-Visual-Transformer

实例教程：(PyTorch)从头实现Transformer
https://colab.research.google.com/drive/1swXWW5sOLW8zSZBaQBYcGQkQ_Bje_bmI

Cross-Modal Contrastive Learning for Text-to-Image Generation
https://arxiv.org/abs/2101.04702

Is Attention Better Than Matrix Decomposition? 
https://openreview.net/forum?id=1FvkSpWosOl

Pre-training without Natural Images
https://arxiv.org/abs/2101.08515

胸片为例的医学图像自动描述
https://towardsdatascience.com/medical-image-captioning-on-chest-x-rays-a43561a6871d

图：完全理解Transformer的注意力机制
https://imgur.com/gallery/vuw15aL

### 视频描述生成相关文献列表
https://github.com/tgc1997/Awesome-Video-Captioning

Keras实例：Vision Transformer图像分类
https://keras.io/examples/vision/image_classification_with_vision_transformer/

### CPTR: Full Transformer Network for Image Captioning
https://arxiv.org/abs/2101.10804

(Colab)基于CLIP的Unsplash图片语义搜索
https://github.com/haltakov/natural-language-image-search

Language-Mediated, Object-Centric Representation Learning
https://arxiv.org/abs/2012.15814

Vx2Text: End-to-End Learning of Video-Based Text Generation From Multimodal Inputs
https://arxiv.org/abs/2101.12059

Bottleneck Transformers for Visual Recognition
https://arxiv.org/abs/2101.11605

用TensorFlow Serving快速部署Transformers
https://huggingface.co/blog/tf-serving

Video Transformer Network
https://arxiv.org/abs/2102.00719

Decoupling the Role of Data, Attention, and Losses in Multimodal Transformers
https://arxiv.org/abs/2102.00529

(Colab)结合CLIP的文本-图像生成
https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf

### Learning to Discretely Compose Reasoning Module Networks for Video Captioning
https://github.com/tgc1997/RMN

Comprehensive Image Captioning via Scene Graph Decomposition
https://github.com/YiwuZhong/Sub-GC

Look and Modify: Modification Networks for Image Captioning
https://github.com/fawazsammani/look-and-modify

Understanding the Difficulty of Training Transformers
https://github.com/LiyuanLucasLiu/Transformer-Clinic

Latent Normalizing Flows for Many-to-Many Cross Domain Mappings 
https://github.com/visinf/lnfmm

### 用编程方式理解Transformers
https://towardsdatascience.com/understanding-transformers-the-programming-way-f8ed22d112b2

transformer的十七大变种优化
https://mp.weixin.qq.com/s/k5XkAwcXkyi9ymFUIej_iw

Transformers与图神经网络
https://github.com/chaitjo/gated-graph-transformers

### Transformer语言模型可视化解析
https://jalammar.github.io/explaining-transformers/

Zelda Rose：Transformer模型训练器，支持分布式训练
https://github.com/LoicGrobol/zeldarose

Unifying Vision-and-Language Tasks via Text Generation
https://github.com/j-min/VL-T5

Source code for "Bi-modal Transformer for Dense Video Captioning"
https://github.com/v-iashin/BMT

Visual Relation Grounding in Videos
https://github.com/doc-doc/vRGV

MMT: Multi-modal Transformer for Video Retrieval
https://github.com/gabeur/mmt

Research code for EMNLP 2020 paper "HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training"
https://github.com/linjieli222/HERO

Zelda Rose：Transformer模型训练器，支持分布式训练
https://github.com/LoicGrobol/zeldarose

Implementation of Vision Transformer in PyTorch, a new model to achieve SOTA in vision classification with using transformer style encoders
https://github.com/gupta-abhay/ViT

Data Movement Is All You Need: A Case Study on Optimizing Transformers.
https://github.com/spcl/substation

### VisualCOMET: Reasoning about the Dynamic Context of a Still Image
https://github.com/jamespark3922/visual-comet

This is the official Pytorch implementation of Length-Adaptive Transformer. 
https://github.com/clovaai/length-adaptive-transformer

AR-Net: Adaptive Resolution Network for Efficient Video Understanding
https://github.com/mengyuest/AR-Net

Code base for WaveTransformer: A novel architecture for automated audio captioning
https://github.com/haantran96/wavetransformer

The official source code for the paper Consensus-Aware Visual-Semantic Embedding for Image-Text Matching (ECCV 2020)
https://github.com/BruceW91/CVSE

Research Code for NeurIPS 2020 Spotlight paper "Large-Scale Adversarial Training for Vision-and-Language Representation Learning": LXMERT adversarial training part
https://github.com/zhegan27/LXMERT-AdvTrain

COOT: Cooperative Hierarchical Transformer for Video-Text Representation Learning
https://github.com/gingsi/coot-videotext

A PyTorch implementation of the paper - "Synthesizer: Rethinking Self-Attention in Transformer Models"
https://github.com/10-zin/Synthesizer

iPerceive: Applying Common-Sense Reasoning to Multi-Modal Dense Video Captioning and Video Question Answering
https://github.com/amanchadha/iPerceive

Video Feature Extraction Code for EMNLP 2020 paper "HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training"
https://github.com/linjieli222/HERO_Video_Feature_Extractor

Big Bird: Transformers for Longer Sequences
https://github.com/google-research/bigbird

Code and data for the framework in "Multimodal Pretraining Unmasked: Unifying the Vision and Language BERTs", arXiv preprint arXiv:2011.15124.
https://github.com/e-bug/volta

Self-Supervised Learning by Cross-Modal Audio-Video Clustering
https://github.com/HumamAlwassel/XDC

Multi-Modal Reasoning Graph for Scene-Text Based Fine-Grained Image Classification and Retrieval
https://github.com/AndresPMD/GCN_classification

Implementation of RealFormer using pytorch
https://github.com/cloneofsimo/RealFormer-pytorch

Tensorflow implementation of the Vision Transformer (An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale)
https://github.com/emla2805/vision-transformer

(Pytorch) Visual Transformers: Token-based Image Representation and Processing for Computer Vision:
https://github.com/tahmid0007/VisualTransformers

Code and data for the project "Visually grounded continual learning of compositional semantics"
https://github.com/INK-USC/VisCOLL

### Connecting Vision and Language with Localized Narratives
https://google.github.io/localized-narratives/ https://github.com/google/localized-narratives

An implementation of Performer, a linear attention-based transformer, in Pytorch
https://github.com/lucidrains/performer-pytorch

DSNet: A Flexible Detect-to-Summarize Network for Video Summarization
https://github.com/li-plus/DSNet

Learning to ground explanations of affect for visual art.
https://github.com/optas/artemis

Keras Implementation of Vision Transformer (An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale)
https://github.com/tuvovan/Vision_Transformer_Keras

Implementation of "Multimodal Text Style Transfer for Outdoor Vision-and-Language Navigation"
https://github.com/VegB/VLN-Transformer

Code of paper: A Recurrent Vision-and-Language BERT for Navigation
https://github.com/YicongHong/Recurrent-VLN-BERT

Implementation of Bottleneck Transformer in Pytorch
https://github.com/lucidrains/bottleneck-transformer-pytorch

Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet
https://github.com/yitu-opensource/T2T-ViT

ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision
https://arxiv.org/abs/2102.03334

Learning Spatiotemporal Features via Video and Text Pair Discrimination
https://github.com/MCG-NJU/CPD-Video

The implementation of paper UniVL: A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation.
https://github.com/microsoft/UniVL

Implementation of Feedback Transformer in Pytorch
https://github.com/lucidrains/feedback-transformer-pytorch

code of the paper "Vision-Language Navigation with Multi-granularity Observation and Auxiliary Reasoning Tasks"
https://github.com/ZhuFengdaaa/MG-AuxRN

We rank the 1st in DSTC8 Audio-Visual Scene-Aware Dialog competition. This is the source code for our AAAI2020-DSTC8-AVSD paper "Bridging Text and Video: A Universal Multimodal Transformer for Video-Audio Scene-Aware Dialog".
https://github.com/ictnlp/DSTC8-AVSD

Implementation of TimeSformer, a pure attention-based solution for video classification
https://github.com/lucidrains/TimeSformer-pytorch

Tensorflow implementation of the Vision Transformer (An Image is Worth 16x16 Words: Transformer
https://github.com/ashishpatel26/Vision-Transformer-Keras-Tensorflow-Pytorch-Examples

基于CLIP用自然语言搜索Youtube视频
https://github.com/haltakov/natural-language-youtube-search

Less is More: ClipBERT for Video-and-Language Learning via Sparse Sampling
https://github.com/jayleicn/ClipBERT

Is Space-Time Attention All You Need for Video Understanding?
https://arxiv.org/abs/2102.05095

Training Vision Transformers for Image Retrieval
https://arxiv.org/abs/2102.05644

【自注意力分类图】“A Visual Self-Attention Taxonomy”
https://paperswithcode.com/newsletter/4/

Optimizing Inference Performance of Transformers on CPUs
https://arxiv.org/abs/2102.06621

Transformer计算机视觉应用研究列表
https://github.com/DirtyHarryLYL/Transformer-in-Vision

Transformers Interpret：Transformer模型解释工具
https://github.com/cdpierse/transformers-interpret

我们能否用自然语言模型描述图片视觉差异？最近本组的EACL 2021工作发现，必须从语义上理解单个图片，同时对多个图片进行对比，才可以很好地用文字描述视觉上的细微差别。L2C: Describing Visual Differences Needs Semantic Understanding of Individuals
https://arxiv.org/abs/2102.01860

Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts
https://arxiv.org/abs/2102.08981

Transformer及其应用参考资料汇编
https://elvissaravia.substack.com/p/learn-about-transformers-a-recipe

Inferring spatial relations from textual descriptions of images
https://arxiv.org/abs/2102.00997 https://github.com/ixa-ehu/rec-coco

The source code of ACL 2020 paper: "Cross-Modality Relevance for Reasoning on Language and Vision"
https://arxiv.org/abs/2005.06035 https://github.com/HLR/Cross_Modality_Relevance

Text-to-Image Generation Grounded by Fine-Grained User Attention
https://github.com/google-research/trecs_image_generation

### Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
https://github.com/whai362/PVT

Position Information in Transformers: An Overview
https://arxiv.org/abs/2102.11090

Do Transformer Modifications Transfer Across Implementations and Applications?
https://arxiv.org/abs/2102.11972

### Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
https://github.com/whai362/PVT https://arxiv.org/abs/2102.12122

Zero-Shot Text-to-Image Generation
https://github.com/openai/DALL-E https://arxiv.org/abs/2102.12092

Transformer 一篇就够了 - 记录了学习Transformer过程中的一些疑问和解答，并且实现Transformer的全过程
https://github.com/BSlience/transformer-all-in-one

大规模Transformer图像识别
https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html https://github.com/google-research/vision_transformer

### FastFormers：CPU上的233倍速Transformer推理 
https://medium.com/ai-in-plain-english/fastformers-233x-faster-transformers-inference-on-cpu-4c0b7a720e1

Towards Accurate and Compact Architectures via Neural Architecture Transformer
https://arxiv.org/abs/2102.10301

Do We Really Need Explicit Position Encodings for Vision Transformers?
https://arxiv.org/abs/2102.10882
https://github.com/Meituan-AutoML/CPVT

Transformer及其NLP应用的直观介绍
https://theaisummer.com/transformer/

OmniNet: Omnidirectional Representations from Transformers
https://www.arxiv-vanity.com/papers/2103.01075

Duel-Level Collaborative Transformer for Image Captioning
https://github.com/luo3300612/image-captioning-DLCT

《Coordinate Attention for Efficient Mobile Network Design》(CVPR 2021) 
github.com/Andrew-Qibin/CoordAttention

Energy-Based Learning for Scene Graph Generation
github.com/mods333/energy-based-scene-graph

长程Transformers论文解读
https://huggingface.co/blog/long-range-transformers

Improving Image Captioning Evaluation by Considering Inter References Variance(ACL 2020) 
https://github.com/ck0123/improved-bertscore-for-image-captioning-evaluation

《Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth》
https://github.com/twistedcubic/attention-rank-collapse 

WenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training
https://www.arxiv-vanity.com/papers/2103.06561

Perspectives and Prospects on Transformer Architecture for Cross-Modal Tasks with Language and Vision
https://www.arxiv-vanity.com/papers/2103.04037

基于OpenAI CLIP模型的Unsplash图片语义搜索
github.com/haofanwang/natural-language-joint-query-search

DeepViT: Towards Deeper Vision Transformer
https://www.arxiv-vanity.com/papers/2103.11886

《An Image is Worth 16x16 Words, What is a Video Worth?》
https://www.arxiv-vanity.com/papers/2103.13915
github.com/Alibaba-MIIL/STAM 

Can Vision Transformers Learn without Natural Images?
https://www.arxiv-vanity.com/papers/2103.13023

Meta-DETR: Few-Shot Object Detection via Unified Image-Level Meta-Learning
https://www.arxiv-vanity.com/papers/2103.11731

《Dodrio: Exploring Transformer Models with Interactive Visualization》
github.com/poloclub/dodrio

《A Practical Survey on Faster and Lighter Transformers》
https://www.arxiv-vanity.com/papers/2103.14636

### multimodal：面向VQA和图像描述研究的多模态数据集和视觉特征库，可直接用pip安装
github.com/cdancette/multimodal

《VinVL: Revisiting Visual Representations in Vision-Language Models》(ECCV 2021) 
github.com/pzzhang/VinVL 

### 《Scaling Local Self-Attention For Parameter Efficient Visual Backbones》(2021)
github.com/lucidrains/halonet-pytorch

《RoFormer: Transformer with Rotary Position Embeddings》(2021) 
github.com/ZhuiyiTechnology/roformer

《Swin Transformer: Hierarchical Vision Transformer using Shifted Windows》(2021) 
github.com/microsoft/Swin-Transformer

《Diverse Image Captioning with Context-Object Split Latent Spaces》(NeurIPS 2020) 
github.com/visinf/cos-cvae 

《Self-Attention Attribution: Interpreting Information Interactions Inside Transformer》(AAAI 2021)
github.com/YRdddream/attattr

### 《An Image is Worth 16x16 Words, What is a Video Worth?》(2021) 
github.com/lucidrains/STAM-pytorch 

CvT: Introducing Convolutions to Vision Transformers
https://arxiv.org/abs/2103.15808

ViViT: A Video Vision Transformer
https://www.arxiv-vanity.com/papers/2103.15691

Thinking Fast and Slow: Efficient Text-to-Visual Retrieval with Transformers
https://www.arxiv-vanity.com/papers/2103.16553

Rethinking Spatial Dimensions of Vision Transformers
github.com/naver-ai/pit 

Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding
https://www.arxiv-vanity.com/papers/2104.00298

Going deeper with Image Transformers
https://www.arxiv-vanity.com/papers/2103.17239

视觉Transformer的大爆发
https://paperswithcode.com/newsletter/7/

On the Adversarial Robustness of Visual Transformers
https://www.arxiv-vanity.com/papers/2103.15670

Towards General Purpose Vision Systems
https://www.arxiv-vanity.com/papers/2104.00743

FasterTransformer：更快的FasterTransformer，包括Transformer相关优化、GPT、BERT等
github.com/NVIDIA/FasterTransformer

多种视觉Transformer的PyTorch实现
github.com/rosinality/vision-transformers-pytorch

Language-based Video Editing via Multi-Modal Multi-Level Transformer
https://www.arxiv-vanity.com/papers/2104.01122

LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference
https://www.arxiv-vanity.com/papers/2104.01136

屠榜各大 CV 任务，微软开源的 Swin Transformer 有多强？
https://weibo.com/ttarticle/p/show?id=2309404626713857622141#related
https://www.zhihu.com/question/437495132/answer/1800881612

Swin Transformer: Hierarchical Vision Transformer using Shifted Windows (2021) 
github.com/berniwal/swin-transformer-pytorch 
github.com/SwinTransformer/Swin-Transformer-Object-Detection

Fourier Image Transformer
github.com/juglab/FourierImageTransformer

视觉-语言相关资源大列表
github.com/sangminwoo/awesome-vision-and-language

句子-视频段定位相关文献与基准列表
github.com/Soldelli/Awesome-Temporal-Language-Grounding-in-Videos 

SiT: Self-supervised vIsion Transformer
github.com/Sara-Ahmed/SiT

PyTorchVideo：Facebook面向视频理解研究的深度学习库
github.com/facebookresearch/pytorchvideo 

OpenAI DALLE复现
github.com/kobiso/DALLE-reproduction

LXMERT: Learning Cross-Modality Encoder Representations from Transformers
github.com/yangxuntu/lxmertcatt

TransFG: A Transformer Architecture for Fine-grained Recognition (2021) 
github.com/TACJu/TransFG

Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers(2021) 
github.com/hila-chefer/Transformer-MM-Explainability

UniMoCo: Unsupervised, Semi-Supervised and Full-Supervised Visual Representation Learning
github.com/dddzg/unimoco

ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases 
github.com/facebookresearch/convit

《DeepViT: Towards Deeper Vision Transformer》(2021) 
github.com/zhoudaquan/dvit_repo

《Visual Semantic Role Labeling for Video Understanding》(CVPR 2021)
github.com/TheShadow29/VidSitu 

《Blow the Dog Whistle: A Chinese Dataset for Cant Understanding with Common Sense and World Knowledge》(NAACL 2021) 
github.com/JetRunner/dogwhistle

《Location-Sensitive Visual Recognition with Cross-IOU Loss》(2021)
github.com/Duankaiwen/LSNet 

《CeiT : Convolutional enhanced image Transformer》(2021) 
github.com/rishikksh20/CeiT-pytorch

《Syntax-Aware Action Targeting for Video Captioning》(CVPR 2020) 
github.com/SydCaption/SAAT

VideoGPT: Video Generation using VQ-VAE and Transformers
github.com/wilson1yan/VideoGPT

RoFormer: Enhanced Transformer with Rotary Position Embedding
https://www.arxiv-vanity.com/papers/2104.09864

EET：针对Transformer-based大模型和长序列场景的高性能PyTorch推理插件
github.com/NetEase-FuXi/EET

Lightning Transfomers：基于Pytorch Lightning, Transformers, Hydra做高性能研究的集成界面，提供了灵活接口用于训练和微调最新Transformer模型
github.com/PyTorchLightning/lightning-transformers

NLP实操手册: 基于Transformer的深度学习架构的应用指南(综述)
https://www.arxiv-vanity.com/papers/2104.10640

MaX-DeepLab：面向端到端全景分割的双通路Transformer
https://arxiv.org/abs/2012.00759

Hierarchical Cross-Modal Agent for Robotics Vision-and-Language Navigation
https://www.arxiv-vanity.com/papers/2104.10674

《So-ViT: Mind Visual Tokens for Vision Transformer》(2021) 
github.com/jiangtaoxie/So-ViT

VideoGPT: Video Generation using VQ-VAE and Transformers (2021)
github.com/wilson1yan/VideoGPT

Learning the Best Pooling Strategy for Visual Semantic Embedding (CVPR 2021) 
github.com/woodfrog/vse_infty

Improving Vision-and-Language Navigation with Image-Text Pairs from the Web (ECCV 2020) 
github.com/arjunmajum/vln-bert

Zero-Shot Detection via Vision and Language Knowledge Distillation
https://www.arxiv-vanity.com/papers/2104.13921

面向视觉的Transformer
iaml-it.github.io/posts/2021-04-28-transformers-in-vision/

DINO与PAWS：训练速度提高10倍的自监督Transformer，无监督视觉分割、分类新水准
https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training/

### 交叉描述：图像与文本的语义相似性
https://www.aclweb.org/anthology/2021.eacl-main.249/
https://ai.googleblog.com/2021/05/crisscrossed-captions-semantic.html

OpenAI-CLIP：CLIP的简单PyTorch实现
github.com/moein-shariatnia/OpenAI-CLIP

Transformers、BERT、ELMo、GPT1论文笔记注释版
github.com/shreyansh26/Annotated-ML-Papers

Relative Positional Encoding for Transformers with Linear Complexity
https://www.arxiv-vanity.com/papers/2105.08399/

跨模态对比学习文本-图像生成
https://arxiv.org/abs/2101.04702

CogView: Mastering Text-to-Image Generation via Transformers
github.com/THUDM/CogView

Transformers-Tutorials：HuggingFace Transformers库实例教程集
github.com/NielsRogge/Transformers-Tutorials

train-CLIP：PyTorch Lightning实现的OpenAI's CLIP训练方案
github.com/Zasder3/train-CLIP 

ConViT：用软卷积归纳偏差改进视觉Transformer
https://wandb.ai/wandb_fc/pytorch-image-models/reports/ConViT-Improving-Vision-Transformers-with-Soft-Convolutional-Inductive-Biases--Vmlldzo3MjIxMDk

Less is More: Pay Less Attention in Vision Transformers
https://www.arxiv-vanity.com/papers/2105.14217/

Not All Images are Worth 16x16 Words: Dynamic Vision Transformers with Adaptive Sequence Length
github.com/blackfeather-wang/Dynamic-Vision-Transformer

Token Labeling: Training a 85.4% Top-1 Accuracy Vision Transformer with 56M Parameters on ImageNet
github.com/zihangJiang/TokenLabeling

《Twins: Revisiting the Design of Spatial Attention in Vision Transformers》
github.com/Meituan-AutoML/Twins

LightSeq: A High Performance Inference Library for Transformers
github.com/bytedance/lightseq

《X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers》(EMNLP 2020) 
github.com/allenai/x-lxmert

《ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision》(ICML 2021) 
github.com/dandelin/vilt 

《LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference》
github.com/facebookresearch/LeViT

《Neuro-Symbolic Representations for Video Captioning: A Case for Leveraging Inductive Biases for Vision and Language》(2020) 
github.com/hassanhub/R3Transformer

《Beyond Categorical Label Representations for Image Classification》(ICLR 2021) 
github.com/BoyuanChen/label_representations 
图像描述其实是图像分类的扩展，是否能吸取这篇论文中的想法，在标签表示方面创新？

《Referring Image Segmentation via Cross-Modal Progressive Comprehension》(CVPR 2020) 
github.com/spyflying/CMPC-Refseg

《Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning》(CVPR 2021) 
github.com/researchmm/soho

《Vision Transformers are Robust Learners》(2021) 
github.com/sayakpaul/robustness-vit

《CvT: Introducing Convolutions to Vision Transformers》(2021) 
github.com/microsoft/CvT

《ResT: An Efficient Transformer for Visual Recognition》(2021) 
github.com/wofmanaf/ResT

《CrossViT : Cross-Attention Multi-Scale Vision Transformer for Image Classification》(2021) 
github.com/rishikksh20/CrossViT-pytorch

《Not All Images are Worth 16x16 Words: Dynamic Vision Transformers with Adaptive Sequence Length》(2021) 
github.com/blackfeather-wang/Dynamic-Vision-Transformer

《You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection》(2021) 
github.com/hustvl/YOLOS

《CvT: Introducing Convolutions to Vision Transformers》(2021) 
github.com/leoxiaobin/CvT

DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification
github.com/raoyongming/DynamicViT

An Attention Free Transformer
https://www.arxiv-vanity.com/papers/2105.14103

When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations
https://www.arxiv-vanity.com/papers/2106.01548

教程：如何创建和训练视觉Transformer
https://theaisummer.com/hugging-face-vit/

Semantic Correspondence with Transformers
github.com/SunghwanHong/CATs

Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks
https://www.arxiv-vanity.com/papers/2105.02358/

X-volution: On the unification of convolution and self-attention
https://www.arxiv-vanity.com/papers/2106.02253/

Attention Free Transformer
github.com/rish-16/aft-pytorch

各种注意力机制的PyTorch实现
github.com/xmu-xiaoma666/External-Attention-pytorch

On Improving Adversarial Transferability of Vision Transformers
github.com/Muzammal-Naseer/Improving-Adversarial-Transferability-of-Vision-Transformers 

MERLOT: Multimodal Neural Script Knowledge Models
https://www.arxiv-vanity.com/papers/2106.02636/

Refiner: Refining Self-attention for Vision Transformers
github.com/zhoudaquan/Refiner_ViT

A Survey of Transformers
https://www.arxiv-vanity.com/papers/2106.04554

Scaling Vision Transformers
https://www.arxiv-vanity.com/papers/2106.04560

Demystifying Local Vision Transformer: Sparse Connectivity, Weight Sharing, and Dynamic Weight
https://www.arxiv-vanity.com/papers/2106.04263

Transformed CNNs: recasting pre-trained convolutional layers with self-attention
https://www.arxiv-vanity.com/papers/2106.05795

微软联合UCSB、UCSC、UNC等单位合作推出的视频与语言理解基准测试集 VALUE: Video-and-Language Understanding Evaluation
https://arxiv.org/pdf/2106.04632.pdf

Space-time Mixing Attention for Video Transformer
https://www.arxiv-vanity.com/papers/2106.05968

《CAT: Cross Attention in Vision Transformer》(2021) 
github.com/linhezheng19/CAT

Keras实例：Transformer视频分类
https://keras.io/examples/vision/video_transformers/

Towards General Purpose Vision Systems：面向通用视觉的任务不可知视觉语言架构
github.com/allenai/gpv-1

Invertible Attention
github.com/Schwartz-Zha/InvertibleAttention

《Episodic Transformer for Vision-and-Language Navigation》
github.com/alexpashevich/E.T. 

How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers
https://www.arxiv-vanity.com/papers/2106.10270

Memory-efficient Transformers via Top-k Attention
github.com/ag1988/top_k_attention

XCiT: Cross-Covariance Image Transformers
github.com/facebookresearch/xcit

Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding
https://www.arxiv-vanity.com/papers/2106.12566

Towards Long-Form Video Understanding
github.com/chaoyuaw/lvu

Video Swin Transformer
github.com/SwinTransformer/Video-Swin-Transformer

Early Convolutions Help Transformers See Better
https://www.arxiv-vanity.com/papers/2106.14881/

CLIPDraw: Exploring Text-to-Drawing Synthesis through Language-Image Encoders
https://arxiv.org/abs/2106.14843
https://colab.research.google.com/github/kvfrans/clipdraw/blob/main/clipdraw.ipynb

Stabilizing Deep Q-Learning with ConvNets and Vision Transformers under Data Augmentation
https://arxiv.org/abs/2107.00644

CLIP-It! Language-Guided Video Summarization
https://arxiv.org/abs/2107.00650

Focal Self-attention for Local-Global Interactions in Vision Transformers
https://arxiv.org/abs/2107.00641

CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows
https://arxiv.org/abs/2107.00652

Rethinking Positional Encoding
github.com/osiriszjq/Rethinking-positional-encoding

Long-Short Transformer: Efficient Transformers for Language and Vision
https://arxiv.org/abs/2107.02192

Transformers与卷积网络的结合
https://ai.facebook.com/blog/computer-vision-combining-transformers-and-convolutional-neural-networks
https://arxiv.org/abs/2103.10697

CMT: Convolutional Neural Networks Meet Vision Transformers
https://arxiv.org/abs/2107.06263

VidLanKD: Improving Language Understanding via Video-Distilled Knowledge Transfer
github.com/zinengtang/VidLanKD

From Show to Tell: A Survey on Image Captioning
https://arxiv.org/abs/2107.06912

How Much Can CLIP Benefit Vision-and-Language Tasks?
github.com/clip-vil/CLIP-ViL

Awesome Text-to-Image：文本-图像合成相关资源大列表
github.com/Yutong-Zhou-cv/Awesome-Text-to-Image

《An Empirical Study of Training Self-Supervised Vision Transformers》(2021) 
github.com/CupidJay/MoCov3-pytorch

《Towards Long-Form Video Understanding》(CVPR 2021) 
github.com/chaoyuaw/lvu

《Video Swin Transformer》(2021) 
github.com/SwinTransformer/Video-Swin-Transformer 

《Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer》(2021) 
github.com/mulinmeng/Shuffle-Transformer