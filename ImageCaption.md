Recognition to Cognition Networks https://visualcommonsense.com
https://github.com/rowanz/r2c

CVPR 2018 - Regularizing RNNs for Caption Generation by Reconstructing The Past with The Present
https://github.com/chenxinpeng/ARNet

Beyond Narrative Description: Generating Poetry from Images by Multi-Adversarial Training
https://github.com/bei21/img2poem

Dataset and starting code for visual entailment dataset https://arxiv.org/abs/1811.10582
https://github.com/necla-ml/SNLI-VE

Mapping Images to Scene Graphs with Permutation-Invariant Structured Prediction
https://github.com/shikorab/SceneGraph

### 【视觉/语言预训练模型最新进展】'Recent Advances in Vision and Language PreTrained Models (VL-PTMs)'
https://github.com/yuewang-cuhk/awesome-vision-language-pretraining-papers

Aligning Visual Regions and Textual Concepts for Semantic-Grounded Image Representations
https://github.com/fenglinliu98/MIA

Tensorflow implementation of "A Structured Self-Attentive Sentence Embedding"
https://github.com/flrngel/Self-Attentive-tensorflow

This repository contains the reference code for the paper Show, Control and Tell: A Framework for Generating Controllable and Grounded Captions (CVPR 2019).
https://github.com/aimagelab/show-control-and-tell 

The Code for ICME2019 Grand Challenge: Short Video Understanding (Single Model Ranks 6th)
https://github.com/guoday/ICME2019-CTR

【基于Transformer的图像自动描述PyTorch/Fairseq扩展】
https://github.com/krasserm/fairseq-image-captioning

Code for Neural Inverse Knitting: From Images to Manufacturing Instructions
https://github.com/xionluhnis/neural_inverse_knitting

Code for paper "Attention on Attention for Image Captioning". ICCV 2019 https://arxiv.org/abs/1908.06954
https://github.com/husthuaan/AoANet

Learning to Evaluate Image Captioning. CVPR 2018
https://github.com/richardaecn/cvpr18-caption-eval

Vision-Language Pre-training for Image Captioning and Question Answering
https://github.com/LuoweiZhou/VLP

Official Tensorflow Implementation of the paper "Bidirectional Attentive Fusion with Context Gating for Dense Video Captioning" in CVPR 2018, with code, model and prediction results.
https://github.com/JaywongWang/DenseVideoCaptioning

A PyTorch implementation of Transformer in "Attention is All You Need" https://arxiv.org/abs/1706.03762
https://github.com/dreamgonfly/Transformer-pytorch

《ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data》
https://www.arxiv-vanity.com/papers/2001.07966/

【结合BERT的图片描述生成】’Image Captioning System - BERT + Image Captioning'
https://github.com/ajamjoom/Image-Captions

M^2: Meshed-Memory Transformer for Image Captioning
https://github.com/aimagelab/meshed-memory-transformer

### Video Grounding and Captioning
https://github.com/facebookresearch/grounded-video-description

Reformer：高效的Transformer
https://github.com/google/trax/tree/master/trax/models/reformer

ICCV研讨会的中英文视频描述大赛
http://vatex.org/main/index.html

Cooperative Vision-and-Dialog Navigation
https://github.com/mmurray/cvdn

Auto-Encoding Scene Graphs for Image Captioning, CVPR 2019
https://github.com/yangxuntu/SGAE

A PyTorch implementation of the Transformer model from "Attention Is All You Need".
https://github.com/phohenecker/pytorch-transformer

【MMF：基于PyTorch的视觉/语言研究模块化框架，可方便进行VQA、图像描述、视觉对话、仇恨检测和其他视觉/语言任务的研究】
https://github.com/facebookresearch/mmf

基于transformers的图像Instagram标题生成
https://github.com/antoninodimaggio/Hugging-Captions

Implementation of 'X-Linear Attention Networks for Image Captioning' [CVPR 2020]
https://github.com/JDAI-CV/image-captioning

Train Scene Graph Generation for Visual Genome and GQA in PyTorch >= 1.2 with improved zero and few-shot generalization. Paper: "Graph Density-Aware Losses for Novel Compositions in Scene Graph Generation"
https://github.com/bknyaz/sgg

Code and Resources for the Transformer Encoder Reasoning Network (TERN) - https://arxiv.org/abs/2004.09144
https://github.com/mesnico/TERN

Code for ACL 2020 paper "Dense-Caption Matching and Frame-Selection Gating for Temporal Localization in VideoQA."
https://github.com/hyounghk/VideoQADenseCapFrameGate-ACL2020

[ACL 2020] PyTorch code for MART: Memory-Augmented Recurrent Transformer for Coherent Video Paragraph Captioning
https://github.com/jayleicn/recurrent-transformer

PyTorch code for: Learning to Generate Grounded Visual Captions without Localization Supervision
https://github.com/chihyaoma/cyclical-visual-captioning

Say As You Wish: Fine-grained Control of Image Caption Generation with Abstract Scene Graphs
https://github.com/cshizhe/asg2cap

Fine-grained Video-Text Retrieval with Hierarchical Graph Reasoning
https://github.com/cshizhe/hgr_v2t

The repository of ECCV 2020 paper Active Visual Information Gathering for Vision-Language Navigation
https://github.com/HanqingWangAI/Active_VLN

Code for the CVPR 2020 oral paper: Weakly Supervised Visual Semantic Parsing
https://github.com/alirezazareian/vspnet https://arxiv.org/abs/2001.02359

Learning Visual Representations with Caption Annotations
https://arxiv.org/abs/2008.01392


Towards Unique and Informative Captioning of Images
https://github.com/princetonvisualai/SPICE-U

计算机如何做到“看图说话”？在没有对应的训练数据的情况下，模型能否准确描述测试图像中新出现的各种类别的物体？微软 Azure 认知服务团队和微软研究院的研究员提出了全新解决方案视觉词表预训练 (Visual Vocabulary Pre-training)。该方法在 nocaps 挑战中取得了新的 SOTA，并首次超越人类表现
https://weibo.com/ttarticle/p/show?id=2309404559989682602061

一个视频的文本摘要生成项目，输入一段视频，通过深度学习网络和人工智能程序识别视频主要表达的意思
https://github.com/CaptainEven/VideoCaption

An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale
https://github.com/google-research/vision_transformer

### CapWAP: Captioning with a Purpose
https://arxiv.org/abs/2011.04264
https://github.com/google-research/language/tree/master/language/capwap

Transformer视觉表示学习文献资源列表
https://github.com/alohays/awesome-visual-representation-learning-with-transformers

Online Bag-of-Visual-Words Generation for Unsupervised Representation Learning
https://github.com/valeoai/obow

VinVL: Making Visual Representations Matter in Vision-Language Models
https://arxiv.org/abs/2101.00529

Cross-Document Language Modeling
https://github.com/aviclu/CD-LM https://arxiv.org/abs/2101.00406

Transformers in Vision: A Survey
https://arxiv.org/abs/2101.01169

视觉Transformer相关工作列表
https://github.com/dk-liang/Awesome-Visual-Transformer

实例教程：(PyTorch)从头实现Transformer
https://colab.research.google.com/drive/1swXWW5sOLW8zSZBaQBYcGQkQ_Bje_bmI

Cross-Modal Contrastive Learning for Text-to-Image Generation
https://arxiv.org/abs/2101.04702

Is Attention Better Than Matrix Decomposition? 
https://openreview.net/forum?id=1FvkSpWosOl

Pre-training without Natural Images
https://arxiv.org/abs/2101.08515

胸片为例的医学图像自动描述
https://towardsdatascience.com/medical-image-captioning-on-chest-x-rays-a43561a6871d

图：完全理解Transformer的注意力机制
https://imgur.com/gallery/vuw15aL

### 视频描述生成相关文献列表
https://github.com/tgc1997/Awesome-Video-Captioning

Keras实例：Vision Transformer图像分类
https://keras.io/examples/vision/image_classification_with_vision_transformer/

### CPTR: Full Transformer Network for Image Captioning
https://arxiv.org/abs/2101.10804

(Colab)基于CLIP的Unsplash图片语义搜索
https://github.com/haltakov/natural-language-image-search

Language-Mediated, Object-Centric Representation Learning
https://arxiv.org/abs/2012.15814

Vx2Text: End-to-End Learning of Video-Based Text Generation From Multimodal Inputs
https://arxiv.org/abs/2101.12059

Bottleneck Transformers for Visual Recognition
https://arxiv.org/abs/2101.11605

用TensorFlow Serving快速部署Transformers
https://huggingface.co/blog/tf-serving

Video Transformer Network
https://arxiv.org/abs/2102.00719

Decoupling the Role of Data, Attention, and Losses in Multimodal Transformers
https://arxiv.org/abs/2102.00529

(Colab)结合CLIP的文本-图像生成
https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf

### Learning to Discretely Compose Reasoning Module Networks for Video Captioning
https://github.com/tgc1997/RMN

Comprehensive Image Captioning via Scene Graph Decomposition
https://github.com/YiwuZhong/Sub-GC

Look and Modify: Modification Networks for Image Captioning
https://github.com/fawazsammani/look-and-modify

Understanding the Difficulty of Training Transformers
https://github.com/LiyuanLucasLiu/Transformer-Clinic

Latent Normalizing Flows for Many-to-Many Cross Domain Mappings 
https://github.com/visinf/lnfmm

### 用编程方式理解Transformers
https://towardsdatascience.com/understanding-transformers-the-programming-way-f8ed22d112b2

transformer的十七大变种优化
https://mp.weixin.qq.com/s/k5XkAwcXkyi9ymFUIej_iw

Transformers与图神经网络
https://github.com/chaitjo/gated-graph-transformers

### Transformer语言模型可视化解析
https://jalammar.github.io/explaining-transformers/

Zelda Rose：Transformer模型训练器，支持分布式训练
https://github.com/LoicGrobol/zeldarose

Unifying Vision-and-Language Tasks via Text Generation
https://github.com/j-min/VL-T5

Source code for "Bi-modal Transformer for Dense Video Captioning"
https://github.com/v-iashin/BMT

Visual Relation Grounding in Videos
https://github.com/doc-doc/vRGV

MMT: Multi-modal Transformer for Video Retrieval
https://github.com/gabeur/mmt

Research code for EMNLP 2020 paper "HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training"
https://github.com/linjieli222/HERO

Zelda Rose：Transformer模型训练器，支持分布式训练
https://github.com/LoicGrobol/zeldarose

Implementation of Vision Transformer in PyTorch, a new model to achieve SOTA in vision classification with using transformer style encoders
https://github.com/gupta-abhay/ViT

Data Movement Is All You Need: A Case Study on Optimizing Transformers.
https://github.com/spcl/substation

### VisualCOMET: Reasoning about the Dynamic Context of a Still Image
https://github.com/jamespark3922/visual-comet

This is the official Pytorch implementation of Length-Adaptive Transformer. 
https://github.com/clovaai/length-adaptive-transformer

AR-Net: Adaptive Resolution Network for Efficient Video Understanding
https://github.com/mengyuest/AR-Net

Code base for WaveTransformer: A novel architecture for automated audio captioning
https://github.com/haantran96/wavetransformer

The official source code for the paper Consensus-Aware Visual-Semantic Embedding for Image-Text Matching (ECCV 2020)
https://github.com/BruceW91/CVSE

Research Code for NeurIPS 2020 Spotlight paper "Large-Scale Adversarial Training for Vision-and-Language Representation Learning": LXMERT adversarial training part
https://github.com/zhegan27/LXMERT-AdvTrain

COOT: Cooperative Hierarchical Transformer for Video-Text Representation Learning
https://github.com/gingsi/coot-videotext

A PyTorch implementation of the paper - "Synthesizer: Rethinking Self-Attention in Transformer Models"
https://github.com/10-zin/Synthesizer

iPerceive: Applying Common-Sense Reasoning to Multi-Modal Dense Video Captioning and Video Question Answering
https://github.com/amanchadha/iPerceive

Video Feature Extraction Code for EMNLP 2020 paper "HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training"
https://github.com/linjieli222/HERO_Video_Feature_Extractor

Big Bird: Transformers for Longer Sequences
https://github.com/google-research/bigbird

Code and data for the framework in "Multimodal Pretraining Unmasked: Unifying the Vision and Language BERTs", arXiv preprint arXiv:2011.15124.
https://github.com/e-bug/volta

Self-Supervised Learning by Cross-Modal Audio-Video Clustering
https://github.com/HumamAlwassel/XDC

Multi-Modal Reasoning Graph for Scene-Text Based Fine-Grained Image Classification and Retrieval
https://github.com/AndresPMD/GCN_classification

Implementation of RealFormer using pytorch
https://github.com/cloneofsimo/RealFormer-pytorch

Tensorflow implementation of the Vision Transformer (An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale)
https://github.com/emla2805/vision-transformer

(Pytorch) Visual Transformers: Token-based Image Representation and Processing for Computer Vision:
https://github.com/tahmid0007/VisualTransformers

Code and data for the project "Visually grounded continual learning of compositional semantics"
https://github.com/INK-USC/VisCOLL

### Connecting Vision and Language with Localized Narratives
https://google.github.io/localized-narratives/ https://github.com/google/localized-narratives

An implementation of Performer, a linear attention-based transformer, in Pytorch
https://github.com/lucidrains/performer-pytorch

DSNet: A Flexible Detect-to-Summarize Network for Video Summarization
https://github.com/li-plus/DSNet

Learning to ground explanations of affect for visual art.
https://github.com/optas/artemis

Keras Implementation of Vision Transformer (An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale)
https://github.com/tuvovan/Vision_Transformer_Keras

Implementation of "Multimodal Text Style Transfer for Outdoor Vision-and-Language Navigation"
https://github.com/VegB/VLN-Transformer

Code of paper: A Recurrent Vision-and-Language BERT for Navigation
https://github.com/YicongHong/Recurrent-VLN-BERT

Implementation of Bottleneck Transformer in Pytorch
https://github.com/lucidrains/bottleneck-transformer-pytorch

Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet
https://github.com/yitu-opensource/T2T-ViT

ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision
https://arxiv.org/abs/2102.03334

Learning Spatiotemporal Features via Video and Text Pair Discrimination
https://github.com/MCG-NJU/CPD-Video

The implementation of paper UniVL: A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation.
https://github.com/microsoft/UniVL

Implementation of Feedback Transformer in Pytorch
https://github.com/lucidrains/feedback-transformer-pytorch

code of the paper "Vision-Language Navigation with Multi-granularity Observation and Auxiliary Reasoning Tasks"
https://github.com/ZhuFengdaaa/MG-AuxRN

We rank the 1st in DSTC8 Audio-Visual Scene-Aware Dialog competition. This is the source code for our AAAI2020-DSTC8-AVSD paper "Bridging Text and Video: A Universal Multimodal Transformer for Video-Audio Scene-Aware Dialog".
https://github.com/ictnlp/DSTC8-AVSD

Implementation of TimeSformer, a pure attention-based solution for video classification
https://github.com/lucidrains/TimeSformer-pytorch

Tensorflow implementation of the Vision Transformer (An Image is Worth 16x16 Words: Transformer
https://github.com/ashishpatel26/Vision-Transformer-Keras-Tensorflow-Pytorch-Examples

基于CLIP用自然语言搜索Youtube视频
https://github.com/haltakov/natural-language-youtube-search

Less is More: ClipBERT for Video-and-Language Learning via Sparse Sampling
https://github.com/jayleicn/ClipBERT

Is Space-Time Attention All You Need for Video Understanding?
https://arxiv.org/abs/2102.05095

Training Vision Transformers for Image Retrieval
https://arxiv.org/abs/2102.05644

【自注意力分类图】“A Visual Self-Attention Taxonomy”
https://paperswithcode.com/newsletter/4/

Optimizing Inference Performance of Transformers on CPUs
https://arxiv.org/abs/2102.06621

Transformer计算机视觉应用研究列表
https://github.com/DirtyHarryLYL/Transformer-in-Vision

Transformers Interpret：Transformer模型解释工具
https://github.com/cdpierse/transformers-interpret

我们能否用自然语言模型描述图片视觉差异？最近本组的EACL 2021工作发现，必须从语义上理解单个图片，同时对多个图片进行对比，才可以很好地用文字描述视觉上的细微差别。L2C: Describing Visual Differences Needs Semantic Understanding of Individuals
https://arxiv.org/abs/2102.01860

Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts
https://arxiv.org/abs/2102.08981

Transformer及其应用参考资料汇编
https://elvissaravia.substack.com/p/learn-about-transformers-a-recipe

Inferring spatial relations from textual descriptions of images
https://arxiv.org/abs/2102.00997 https://github.com/ixa-ehu/rec-coco

The source code of ACL 2020 paper: "Cross-Modality Relevance for Reasoning on Language and Vision"
https://arxiv.org/abs/2005.06035 https://github.com/HLR/Cross_Modality_Relevance

Text-to-Image Generation Grounded by Fine-Grained User Attention
https://github.com/google-research/trecs_image_generation

### Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
https://github.com/whai362/PVT

Position Information in Transformers: An Overview
https://arxiv.org/abs/2102.11090

Do Transformer Modifications Transfer Across Implementations and Applications?
https://arxiv.org/abs/2102.11972

### Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
https://github.com/whai362/PVT https://arxiv.org/abs/2102.12122

Zero-Shot Text-to-Image Generation
https://github.com/openai/DALL-E https://arxiv.org/abs/2102.12092

Transformer 一篇就够了 - 记录了学习Transformer过程中的一些疑问和解答，并且实现Transformer的全过程
https://github.com/BSlience/transformer-all-in-one

大规模Transformer图像识别
https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html https://github.com/google-research/vision_transformer

### FastFormers：CPU上的233倍速Transformer推理 
https://medium.com/ai-in-plain-english/fastformers-233x-faster-transformers-inference-on-cpu-4c0b7a720e1

Towards Accurate and Compact Architectures via Neural Architecture Transformer
https://arxiv.org/abs/2102.10301

Do We Really Need Explicit Position Encodings for Vision Transformers?
https://arxiv.org/abs/2102.10882
https://github.com/Meituan-AutoML/CPVT

Transformer及其NLP应用的直观介绍
https://theaisummer.com/transformer/