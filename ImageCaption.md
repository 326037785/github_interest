Recognition to Cognition Networks https://visualcommonsense.com
https://github.com/rowanz/r2c

CVPR 2018 - Regularizing RNNs for Caption Generation by Reconstructing The Past with The Present
https://github.com/chenxinpeng/ARNet

Beyond Narrative Description: Generating Poetry from Images by Multi-Adversarial Training
https://github.com/bei21/img2poem

Dataset and starting code for visual entailment dataset https://arxiv.org/abs/1811.10582
https://github.com/necla-ml/SNLI-VE

Mapping Images to Scene Graphs with Permutation-Invariant Structured Prediction
https://github.com/shikorab/SceneGraph

### 【视觉/语言预训练模型最新进展】'Recent Advances in Vision and Language PreTrained Models (VL-PTMs)'
https://github.com/yuewang-cuhk/awesome-vision-language-pretraining-papers

Aligning Visual Regions and Textual Concepts for Semantic-Grounded Image Representations
https://github.com/fenglinliu98/MIA

Tensorflow implementation of "A Structured Self-Attentive Sentence Embedding"
https://github.com/flrngel/Self-Attentive-tensorflow

This repository contains the reference code for the paper Show, Control and Tell: A Framework for Generating Controllable and Grounded Captions (CVPR 2019).
https://github.com/aimagelab/show-control-and-tell 

The Code for ICME2019 Grand Challenge: Short Video Understanding (Single Model Ranks 6th)
https://github.com/guoday/ICME2019-CTR

【基于Transformer的图像自动描述PyTorch/Fairseq扩展】
https://github.com/krasserm/fairseq-image-captioning

Code for Neural Inverse Knitting: From Images to Manufacturing Instructions
https://github.com/xionluhnis/neural_inverse_knitting

Code for paper "Attention on Attention for Image Captioning". ICCV 2019 https://arxiv.org/abs/1908.06954
https://github.com/husthuaan/AoANet

Learning to Evaluate Image Captioning. CVPR 2018
https://github.com/richardaecn/cvpr18-caption-eval

Vision-Language Pre-training for Image Captioning and Question Answering
https://github.com/LuoweiZhou/VLP

Official Tensorflow Implementation of the paper "Bidirectional Attentive Fusion with Context Gating for Dense Video Captioning" in CVPR 2018, with code, model and prediction results.
https://github.com/JaywongWang/DenseVideoCaptioning

A PyTorch implementation of Transformer in "Attention is All You Need" https://arxiv.org/abs/1706.03762
https://github.com/dreamgonfly/Transformer-pytorch

《ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data》
https://www.arxiv-vanity.com/papers/2001.07966/

【结合BERT的图片描述生成】’Image Captioning System - BERT + Image Captioning'
https://github.com/ajamjoom/Image-Captions

M^2: Meshed-Memory Transformer for Image Captioning
https://github.com/aimagelab/meshed-memory-transformer

### Video Grounding and Captioning
https://github.com/facebookresearch/grounded-video-description

Reformer：高效的Transformer
https://github.com/google/trax/tree/master/trax/models/reformer

ICCV研讨会的中英文视频描述大赛
http://vatex.org/main/index.html

Cooperative Vision-and-Dialog Navigation
https://github.com/mmurray/cvdn

Auto-Encoding Scene Graphs for Image Captioning, CVPR 2019
https://github.com/yangxuntu/SGAE

A PyTorch implementation of the Transformer model from "Attention Is All You Need".
https://github.com/phohenecker/pytorch-transformer

【MMF：基于PyTorch的视觉/语言研究模块化框架，可方便进行VQA、图像描述、视觉对话、仇恨检测和其他视觉/语言任务的研究】
https://github.com/facebookresearch/mmf

基于transformers的图像Instagram标题生成
https://github.com/antoninodimaggio/Hugging-Captions

Implementation of 'X-Linear Attention Networks for Image Captioning' [CVPR 2020]
https://github.com/JDAI-CV/image-captioning

Train Scene Graph Generation for Visual Genome and GQA in PyTorch >= 1.2 with improved zero and few-shot generalization. Paper: "Graph Density-Aware Losses for Novel Compositions in Scene Graph Generation"
https://github.com/bknyaz/sgg

Code and Resources for the Transformer Encoder Reasoning Network (TERN) - https://arxiv.org/abs/2004.09144
https://github.com/mesnico/TERN

Code for ACL 2020 paper "Dense-Caption Matching and Frame-Selection Gating for Temporal Localization in VideoQA."
https://github.com/hyounghk/VideoQADenseCapFrameGate-ACL2020

[ACL 2020] PyTorch code for MART: Memory-Augmented Recurrent Transformer for Coherent Video Paragraph Captioning
https://github.com/jayleicn/recurrent-transformer

PyTorch code for: Learning to Generate Grounded Visual Captions without Localization Supervision
https://github.com/chihyaoma/cyclical-visual-captioning

Say As You Wish: Fine-grained Control of Image Caption Generation with Abstract Scene Graphs
https://github.com/cshizhe/asg2cap

Fine-grained Video-Text Retrieval with Hierarchical Graph Reasoning
https://github.com/cshizhe/hgr_v2t

The repository of ECCV 2020 paper `Active Visual Information Gathering for Vision-Language Navigation
https://github.com/HanqingWangAI/Active_VLN

Code for the CVPR 2020 oral paper: Weakly Supervised Visual Semantic Parsing
https://github.com/alirezazareian/vspnet https://arxiv.org/abs/2001.02359

Learning Visual Representations with Caption Annotations
https://arxiv.org/abs/2008.01392


Towards Unique and Informative Captioning of Images
https://github.com/princetonvisualai/SPICE-U

计算机如何做到“看图说话”？在没有对应的训练数据的情况下，模型能否准确描述测试图像中新出现的各种类别的物体？微软 Azure 认知服务团队和微软研究院的研究员提出了全新解决方案视觉词表预训练 (Visual Vocabulary Pre-training)。该方法在 nocaps 挑战中取得了新的 SOTA，并首次超越人类表现
https://weibo.com/ttarticle/p/show?id=2309404559989682602061

一个视频的文本摘要生成项目，输入一段视频，通过深度学习网络和人工智能程序识别视频主要表达的意思
https://github.com/CaptainEven/VideoCaption

An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale
https://github.com/google-research/vision_transformer

### CapWAP: Captioning with a Purpose
https://arxiv.org/abs/2011.04264
https://github.com/google-research/language/tree/master/language/capwap

Transformer视觉表示学习文献资源列表
https://github.com/alohays/awesome-visual-representation-learning-with-transformers

Online Bag-of-Visual-Words Generation for Unsupervised Representation Learning
https://github.com/valeoai/obow

VinVL: Making Visual Representations Matter in Vision-Language Models
https://arxiv.org/abs/2101.00529

Cross-Document Language Modeling
https://github.com/aviclu/CD-LM https://arxiv.org/abs/2101.00406

Transformers in Vision: A Survey
https://arxiv.org/abs/2101.01169

视觉Transformer相关工作列表
https://github.com/dk-liang/Awesome-Visual-Transformer

实例教程：(PyTorch)从头实现Transformer
https://colab.research.google.com/drive/1swXWW5sOLW8zSZBaQBYcGQkQ_Bje_bmI

Cross-Modal Contrastive Learning for Text-to-Image Generation
https://arxiv.org/abs/2101.04702

Is Attention Better Than Matrix Decomposition? 
https://openreview.net/forum?id=1FvkSpWosOl

Pre-training without Natural Images
https://arxiv.org/abs/2101.08515

胸片为例的医学图像自动描述
https://towardsdatascience.com/medical-image-captioning-on-chest-x-rays-a43561a6871d

图：完全理解Transformer的注意力机制
https://imgur.com/gallery/vuw15aL

### 视频描述生成相关文献列表
https://github.com/tgc1997/Awesome-Video-Captioning

Keras实例：Vision Transformer图像分类
https://keras.io/examples/vision/image_classification_with_vision_transformer/

### CPTR: Full Transformer Network for Image Captioning
https://arxiv.org/abs/2101.10804

(Colab)基于CLIP的Unsplash图片语义搜索
https://github.com/haltakov/natural-language-image-search

Language-Mediated, Object-Centric Representation Learning
https://arxiv.org/abs/2012.15814

Vx2Text: End-to-End Learning of Video-Based Text Generation From Multimodal Inputs
https://arxiv.org/abs/2101.12059

Bottleneck Transformers for Visual Recognition
https://arxiv.org/abs/2101.11605

用TensorFlow Serving快速部署Transformers
https://huggingface.co/blog/tf-serving

Video Transformer Network
https://arxiv.org/abs/2102.00719

Decoupling the Role of Data, Attention, and Losses in Multimodal Transformers
https://arxiv.org/abs/2102.00529

(Colab)结合CLIP的文本-图像生成
https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf

### Learning to Discretely Compose Reasoning Module Networks for Video Captioning
https://github.com/tgc1997/RMN

Comprehensive Image Captioning via Scene Graph Decomposition
https://github.com/YiwuZhong/Sub-GC

Look and Modify: Modification Networks for Image Captioning
https://github.com/fawazsammani/look-and-modify

Understanding the Difficulty of Training Transformers
https://github.com/LiyuanLucasLiu/Transformer-Clinic

Latent Normalizing Flows for Many-to-Many Cross Domain Mappings 
https://github.com/visinf/lnfmm